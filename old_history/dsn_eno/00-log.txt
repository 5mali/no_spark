**************************************************************
00: The ORIGINAL START
**************************************************************
C(RELU)
|
|--C1(Huber Loss)--X
|              
|--C2(Leaky RELU)--X
|
|--C3(Xa-Ka-Ka)--X                
|                                 
|--C4(Ka-Ka-Xa)->C4A(LR=0.0001)->C4B--->0C0(No change during last phase)--OK
                                  |     |
                                  |     |                               
                                  |     |
                                  |     |-->OC3(DEPTH=3)->0C3A(Disruptive Training)---------------->00
                                  |     |
                                  |     |-->OC2(WIDTH=50)->0C2A(Disruptive Training)=0C0A1--OK <-|
                                  |                                                              |
                                  |                                                              |
                                  |--->0C--------------OK                                        |
                                  |    |                                                     EQUIVALENT
                                  |    0C1(WIDTH=50) --OK                                        |
                                  |                                                              |
                                  |                                                              |
                                  |---->OC0A(Disruptive)-->0C0A1(WIDTH=50)------------------OK<--|
                                  

HYPOTHESIS:

- Disruptive training with sawtooth EPSILON ranging from 0.5 to 0.95 to make model more robust
- Increase DEPTH so that network learns better during disruption

MODEL:
0C3A = 00 : INPUT->FC1->RELU->FC2->RELU->FC3->RELU->OUT
LOSS : MSE

LEARNING:   INIT_WEIGHT     = FC1 : KAIMING
                              FC2 : KAIMING
                              FC3 : KAIMING
                              OUT : XAVIER
            WIDTH           = 20
            DEPTH           = 3
            WEIGHT_DECAY    = 1E-3
            LR              = 1E-4
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = SAWTOOTH (0.5-0.95)              
            GAMMA           = 0.9                
            LAMBDA          = 0.9
            
TRAINING:   TOKYO[2000-2009]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
            
            LAST TRAINING ITERATIONS = 10 [TOKYO, 2000-2009]
            EPSILON                  = 0.95
            LR                       = 1E-4
            
TESTING:    TOKYO[2010-2018]
            GREEDY POLICY
   
*********
RESULTS:
*********
SEED 0
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.42 		 0
2012 		 1.33 		 2
2013 		 1.36 		 1
2014 		 1.31 		 1
2015 		 1.31 		 0
2016 		 1.35 		 1
2017 		 1.39 		 1
2018 		 1.34 		 0
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.42 		 0
2012 		 1.33 		 2
2013 		 1.36 		 1
2014 		 1.31 		 1
2015 		 1.31 		 0
2016 		 1.35 		 1
2017 		 1.39 		 1
2018 		 1.34 		 0

SEED 1
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.4 		 0
2012 		 1.34 		 0
2013 		 1.39 		 0
2014 		 1.28 		 0
2015 		 1.26 		 0
2016 		 1.3 		 0
2017 		 1.35 		 2
2018 		 1.33 		 0
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.4 		 0
2012 		 1.34 		 0
2013 		 1.39 		 0
2014 		 1.28 		 0
2015 		 1.26 		 0
2016 		 1.3 		 0
2017 		 1.35 		 2
2018 		 1.33 		 0

SEED 2
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.47 		 0
2012 		 1.4 		 0
2013 		 1.46 		 0
2014 		 1.35 		 0
2015 		 1.3 		 0
2016 		 1.36 		 0
2017 		 1.39 		 3
2018 		 1.39 		 0
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.47 		 0
2012 		 1.4 		 0
2013 		 1.46 		 0
2014 		 1.35 		 0
2015 		 1.3 		 0
2016 		 1.36 		 0
2017 		 1.39 		 3
2018 		 1.39 		 0

SEED 3
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.44 		 0
2012 		 1.37 		 0
2013 		 1.4 		 0
2014 		 1.31 		 1
2015 		 1.27 		 0
2016 		 1.34 		 0
2017 		 1.4 		 2
2018 		 1.34 		 0
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.44 		 0
2012 		 1.37 		 0
2013 		 1.4 		 0
2014 		 1.31 		 1
2015 		 1.27 		 0
2016 		 1.34 		 0
2017 		 1.4 		 2
2018 		 1.34 		 0

SEED 4
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.37 		 0
2012 		 1.33 		 0
2013 		 1.36 		 0
2014 		 1.28 		 0
2015 		 1.28 		 0
2016 		 1.3 		 0
2017 		 1.35 		 0
2018 		 1.3 		 0
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.37 		 0
2012 		 1.33 		 0
2013 		 1.36 		 0
2014 		 1.28 		 0
2015 		 1.28 		 0
2016 		 1.3 		 0
2017 		 1.35 		 0
2018 		 1.3 		 0

SEED 5
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.39 		 0
2012 		 1.3 		 1
2013 		 1.36 		 0
2014 		 1.22 		 3
2015 		 1.23 		 0
2016 		 1.29 		 0
2017 		 1.31 		 5
2018 		 1.3 		 0
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.39 		 0
2012 		 1.3 		 1
2013 		 1.36 		 0
2014 		 1.22 		 3
2015 		 1.23 		 0
2016 		 1.29 		 0
2017 		 1.31 		 5
2018 		 1.3 		 0

SEED 6
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.34 		 0
2012 		 1.28 		 0
2013 		 1.33 		 0
2014 		 1.23 		 1
2015 		 1.25 		 0
2016 		 1.27 		 1
2017 		 1.37 		 1
2018 		 1.28 		 0
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.34 		 0
2012 		 1.28 		 0
2013 		 1.33 		 0
2014 		 1.23 		 1
2015 		 1.25 		 0
2016 		 1.27 		 1
2017 		 1.37 		 1
2018 		 1.28 		 0

***************************
DISCUSSION AND CONCLUSIONS
***************************
Good stable results. Using this as a baseline