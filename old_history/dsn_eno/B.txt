**************************************************************
B
**************************************************************

                       
HYPOTHESIS:
-Try Sigmoid Activations in all layers

- Increase WIDTH/DEPTH so that network learns better during disruption
This is so that catastrophic results don't appear.

- Disruptive training with sawtooth EPSILON ranging from 0.5 to 0.95 to make model more robust


MODEL:
B : INPUT->FC1->SIGMOID->FC2->SIGMOID->FC3->SIGMOID->OUT
LOSS : MSE

LEARNING:   INIT_WEIGHT     = FC1 : XAVIER
                              FC2 : XAVIER
                              FC3 : XAVIER
                              OUT : XAVIER
            WIDTH           = 50
            DEPTH           = 3
            WEIGHT_DECAY    = 1E-3
            LR              = 1E-4
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = SAWTOOTH (0.5-0.95)              
            GAMMA           = 0.9                
            LAMBDA          = 0.9
            
TRAINING:   TOKYO[2000-2009]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
            
            LAST TRAINING ITERATIONS = 10 [TOKYO, 2000-2009]
            EPSILON                  = 0.95
            LR                       = 1E-4
            
TESTING:    TOKYO[2000-2018]
            GREEDY POLICY
   
*********
RESULTS:
*********
SEED 0
YEAR		AVG_RWD		VIOLATIONS
2000 		 1.31 		 0
2001 		 1.28 		 0
2002 		 1.26 		 0
2003 		 1.3 		 1
2004 		 1.29 		 0
2005 		 1.38 		 0
2006 		 1.29 		 0
2007 		 1.28 		 0
2008 		 1.39 		 0
2009 		 1.29 		 0
2010 		 1.26 		 0
2011 		 1.31 		 0
2012 		 1.3 		 0
2013 		 1.27 		 0
2014 		 1.22 		 0
2015 		 1.23 		 0
2016 		 1.28 		 0
2017 		 1.27 		 0
2018 		 1.24 		 0

SEED 1
2000 		 1.39 		 0
2001 		 1.38 		 0
2002 		 1.34 		 0
2003 		 1.36 		 0
2004 		 1.37 		 0
2005 		 1.45 		 0
2006 		 1.35 		 0
2007 		 1.38 		 0
2008 		 1.43 		 0
2009 		 1.38 		 0
2010 		 1.39 		 0
2011 		 1.44 		 0
2012 		 1.42 		 0
2013 		 1.43 		 0
2014 		 1.35 		 0
2015 		 1.35 		 0
2016 		 1.35 		 0
2017 		 1.4 		 0
2018 		 1.37 		 0

SEED 2
2000 		 1.29 		 0
2001 		 1.28 		 0
2002 		 1.25 		 0
2003 		 1.27 		 1
2004 		 1.26 		 1
2005 		 1.36 		 0
2006 		 1.27 		 0
2007 		 1.28 		 1
2008 		 1.36 		 0
2009 		 1.3 		 0
2010 		 1.28 		 0
2011 		 1.34 		 0
2012 		 1.34 		 0
2013 		 1.32 		 0
2014 		 1.24 		 0
2015 		 1.27 		 0
2016 		 1.26 		 0
2017 		 1.29 		 2
2018 		 1.27 		 1

SEED 3
2000 		 1.29 		 0
2001 		 1.26 		 0
2002 		 1.26 		 0
2003 		 1.28 		 0
2004 		 1.26 		 1
2005 		 1.33 		 0
2006 		 1.27 		 0
2007 		 1.27 		 0
2008 		 1.34 		 0
2009 		 1.29 		 0
2010 		 1.28 		 0
2011 		 1.3 		 0
2012 		 1.32 		 0
2013 		 1.29 		 1
2014 		 1.26 		 0
2015 		 1.26 		 0
2016 		 1.24 		 1
2017 		 1.29 		 0
2018 		 1.24 		 2

SEED 4
2000 		 1.23 		 6
2001 		 1.19 		 12
2002 		 1.16 		 10
2003 		 1.3 		 2
2004 		 1.17 		 10
2005 		 1.37 		 2
2006 		 1.29 		 3
2007 		 1.17 		 12
2008 		 1.35 		 3
2009 		 1.23 		 9
2010 		 1.17 		 10
2011 		 1.23 		 12
2012 		 1.2 		 7
2013 		 1.09 		 19
2014 		 1.0 		 19
2015 		 1.08 		 15
2016 		 1.17 		 12
2017 		 1.14 		 18
2018 		 1.04 		 18

SEED 5
000 		 1.24 		 0
2001 		 1.22 		 0
2002 		 1.2 		 1
2003 		 1.24 		 0
2004 		 1.2 		 1
2005 		 1.3 		 0
2006 		 1.22 		 0
2007 		 1.23 		 1
2008 		 1.3 		 0
2009 		 1.23 		 0
2010 		 1.21 		 1
2011 		 1.25 		 1
2012 		 1.28 		 0
2013 		 1.23 		 3
2014 		 1.18 		 1
2015 		 1.2 		 1
2016 		 1.2 		 1
2017 		 1.26 		 0
2018 		 1.18 		 2

SEED 6
2000 		 1.07 		 0
2001 		 1.04 		 0
2002 		 1.03 		 1
2003 		 1.08 		 2
2004 		 0.98 		 6
2005 		 1.15 		 0
2006 		 1.09 		 0
2007 		 1.03 		 3
2008 		 1.15 		 0
2009 		 1.09 		 0
2010 		 0.98 		 6
2011 		 1.04 		 5
2012 		 1.08 		 0
2013 		 1.0 		 7
2014 		 0.92 		 5
2015 		 0.97 		 4
2016 		 1.04 		 5
2017 		 1.0 		 5
2018 		 0.93 		 7

***************************
DISCUSSION AND CONCLUSIONS
***************************
Sigmoid Activations have slower learning but seems better learners. However it is not as robust as RELU.