D       [reward =    ( 1 - np.abs(np.mean(self.atrack)-action)/9 )]
|
|-> D1  [reward = 2* ( 1 - np.abs(np.mean(self.atrack)-action)/9)]---> Least variance
|-> D2  [reward =    ( 1 - np.abs(self.prev_action-action)/9 )]
|-> D3  [reward = 2* ( 1 - np.abs(self.prev_action-action)/9 )]

**************************************************************
D
**************************************************************

                       
HYPOTHESIS:
- Use reward structures that encourage low duty cycle variance

MODEL:
D : INPUT->FC1->RELU->FC2->SIGMOID->FC3->RELU->OUT
LOSS : MSE

LEARNING:   INIT_WEIGHT     = FC1 : KAIMING
                              FC2 : XAVIER
                              FC3 : KAIMING
                              OUT : XAVIER
            WIDTH           = 50
            DEPTH           = 3
            WEIGHT_DECAY    = 1E-3
            LR              = 1E-4
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = SAWTOOTH (0.5-0.95)              
            GAMMA           = 0.9                
            LAMBDA          = 0.9
            
TRAINING:   TOKYO[2000-2009]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
            
            LAST TRAINING ITERATIONS = 10 [TOKYO, 2000-2009]
            EPSILON                  = 0.95
            LR                       = 1E-4
            
TESTING:    TOKYO[2000-2018]
            GREEDY POLICY
   
*********
RESULTS:
*********
D

YEAR		AVG_RWD		VIOLATIONS
2000 		 2.25 		 0
2001 		 2.3 		 0
2002 		 2.12 		 3
2003 		 2.17 		 3
2004 		 2.24 		 0
2005 		 2.34 		 0
2006 		 2.23 		 0
2007 		 2.25 		 0
2008 		 2.28 		 0
2009 		 2.25 		 0
2010 		 2.27 		 0
2011 		 2.31 		 0
2012 		 2.23 		 0
2013 		 2.28 		 0
2014 		 2.22 		 0
2015 		 2.19 		 0
2016 		 2.24 		 0
2017 		 2.28 		 0
2018 		 2.24 		 0

D1
YEAR		AVG_RWD		VIOLATIONS
2000 		 2.88 		 0
2001 		 2.94 		 0
2002 		 2.77 		 2
2003 		 2.86 		 4
2004 		 2.82 		 5
2005 		 3.03 		 1
2006 		 2.91 		 0
2007 		 2.83 		 5
2008 		 2.92 		 2
2009 		 2.94 		 0
2010 		 2.91 		 3
2011 		 2.92 		 5
2012 		 2.89 		 2
2013 		 2.87 		 6
2014 		 2.76 		 5
2015 		 2.79 		 2
2016 		 2.84 		 7
2017 		 2.92 		 6
2018 		 2.79 		 6

D2
YEAR		AVG_RWD		VIOLATIONS
2000 		 2.43 		 0
2001 		 2.46 		 0
2002 		 2.34 		 2
2003 		 2.4 		 2
2004 		 2.41 		 0
2005 		 2.52 		 0
2006 		 2.42 		 0
2007 		 2.42 		 1
2008 		 2.48 		 0
2009 		 2.44 		 0
2010 		 2.44 		 0
2011 		 2.47 		 0
2012 		 2.41 		 1
2013 		 2.45 		 0
2014 		 2.37 		 0
2015 		 2.38 		 0
2016 		 2.42 		 0
2017 		 2.45 		 0
2018 		 2.4 		 1

D3
YEAR		AVG_RWD		VIOLATIONS
2000 		 3.33 		 0
2001 		 3.35 		 0
2002 		 3.27 		 1
2003 		 3.32 		 2
2004 		 3.3 		 3
2005 		 3.4 		 0
2006 		 3.36 		 0
2007 		 3.31 		 2
2008 		 3.39 		 0
2009 		 3.35 		 0
2010 		 3.3 		 2
2011 		 3.33 		 2
2012 		 3.29 		 1
2013 		 3.31 		 1
2014 		 3.22 		 2
2015 		 3.27 		 1
2016 		 3.29 		 3
2017 		 3.34 		 0
2018 		 3.25 		 4

***************************
DISCUSSION AND CONCLUSIONS
***************************
ENP is compromised.
- D has best ENP but D1 has the least duty cycle variance.
- D3 is only slightly better than D2.
- D,D1, D2, D3 both are not intelligent enough to decrease the duty cycles on zero battery days

Using mean of atrack results in better performance.

