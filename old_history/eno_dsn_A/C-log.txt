sn_prime(U-seed0) -> 0U -> A -> A4 -> C

HYPOTHESIS:
Add a RELU layer activation between linear layers

MODEL   :   C : INPUT->FC1->RELU->FC2->RELU->OUT


LEARNING:   INIT_WEIGHT     = normal(0.01)
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.001
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
SEED 0
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.4 		 1
2012 		 1.34 		 0
2013 		 1.35 		 2
2014 		 1.26 		 2
2015 		 1.27 		 0
2016 		 1.31 		 3
2017 		 1.17 		 11
2018 		 1.31 		 2

SEED 1
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.51 		 0
2012 		 1.44 		 1
2013 		 1.45 		 4
2014 		 1.36 		 2
2015 		 1.36 		 1
2016 		 1.4 		 5
2017 		 1.47 		 0
2018 		 1.38 		 5

SEED 2
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.4 		 2
2012 		 1.31 		 2
2013 		 1.32 		 3
2014 		 1.18 		 7
2015 		 1.2 		 3
2016 		 1.27 		 3
2017 		 1.29 		 7
2018 		 1.23 		 5

CONCLUSIONS:
Additional RELU activation helps very much.


DISCUSSION:

**************************************************************
C(RELU) -> C0

HYPOTHESIS:
Check initialization with normal(0.001)
MODEL   :   C : INPUT->FC1->RELU->FC2->RELU->OUT


LEARNING:   INIT_WEIGHT     = normal(0.001)
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.001
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
SEED 0
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.22 		 0
2012 		 1.19 		 0
2013 		 1.21 		 0
2014 		 1.16 		 0
2015 		 1.16 		 0
2016 		 1.18 		 1
2017 		 1.21 		 0
2018 		 1.14 		 0

SEED 1
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.24 		 0
2012 		 1.22 		 1
2013 		 1.23 		 0
2014 		 1.18 		 0
2015 		 1.16 		 1
2016 		 1.18 		 1
2017 		 1.23 		 0
2018 		 1.17 		 0

SEED 2

YEAR		AVG_RWD		VIOLATIONS
2011 		 0.88 		 28
2012 		 0.77 		 29
2013 		 0.77 		 34
2014 		 0.62 		 41
2015 		 0.76 		 32
2016 		 0.94 		 24
2017 		 0.87 		 30
2018 		 0.63 		 42

CONCLUSIONS:
Starting from a lower initialization helps improve performance w.r.t C only in 2/3 cases. In 1/3 case, the results are much worse than C

DISCUSSION:

**************************************************************
C(RELU) -> C0 ->C0A

HYPOTHESIS:
Decrease LR to 0.0001 ot check if the learning is more stable and there are lesser spikes

MODEL   :   C : INPUT->FC1->RELU->FC2->RELU->OUT


LEARNING:   INIT_WEIGHT     = normal(0.001)
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.0001
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
SEED 0
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.31 		 3
2012 		 1.32 		 1
2013 		 1.25 		 7
2014 		 1.21 		 3
2015 		 1.23 		 3
2016 		 1.25 		 4
2017 		 1.27 		 4
2018 		 1.19 		 6

SEED 1
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.23 		 4
2012 		 1.19 		 0
2013 		 1.17 		 4
2014 		 1.05 		 6
2015 		 1.07 		 2
2016 		 1.14 		 4
2017 		 1.21 		 4
2018 		 1.09 		 5

SEED 2
#Learning is extremely slow in this case
YEAR		AVG_RWD		VIOLATIONS
2011 		 0.68 		 32
2012 		 0.43 		 43
2013 		 0.55 		 40
2014 		 0.35 		 43
2015 		 0.21 		 50
2016 		 0.46 		 38
2017 		 0.69 		 29
2018 		 0.46 		 35

CONCLUSIONS:
Decreasing LR with this intialization does not help performance at all. It decrease performance.

DISCUSSION:

**************************************************************

C(RELU) -> C1

HYPOTHESIS:
USE HUBER LOSS

MODEL   :   C1 : INPUT->FC1->RELU->FC2->RELU->OUT
                 LOSS = HUBER LOSS


LEARNING:   INIT_WEIGHT     = normal(0.01)
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.001
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
SEED 0
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.25 		 8
2012 		 1.21 		 10
2013 		 1.14 		 17
2014 		 1.03 		 18
2015 		 1.09 		 18
2016 		 1.21 		 11
2017 		 1.16 		 16
2018 		 1.07 		 18

SEED 1
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.47 		 0
2012 		 1.41 		 0
2013 		 1.42 		 1
2014 		 1.33 		 0
2015 		 1.34 		 0
2016 		 1.39 		 0
2017 		 1.4 		 2
2018 		 1.36 		 1

SEED 2
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.21 		 7
2012 		 1.19 		 4
2013 		 1.12 		 12
2014 		 1.08 		 7
2015 		 1.11 		 7
2016 		 1.2 		 5
2017 		 1.18 		 7
2018 		 1.07 		 12

CONCLUSIONS:
Huber loss can outperfrom MSE Less if initializations are good

DISCUSSION:

**************************************************************

C(RELU) -> C1(Huber)->C1B(LR=0.0005)

HYPOTHESIS:
Decrease LR to see if Huber loss actually makes a difference

MODEL   :   C1B : INPUT->FC1->RELU->FC2->RELU->OUT
                  LOSS = HUBER LOSS


LEARNING:   INIT_WEIGHT     = normal(0.01)
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.0005
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
SEED 0
#slight improvement by decreasing LR
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.33 		 6
2012 		 1.29 		 4
2013 		 1.3 		 6
2014 		 1.17 		 6
2015 		 1.14 		 7
2016 		 1.23 		 3
2017 		 1.2 		 11
2018 		 1.19 		 8

SEED 1
#slight decrease in performance
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.44 		 1
2012 		 1.39 		 1
2013 		 1.36 		 3
2014 		 1.28 		 3
2015 		 1.29 		 1
2016 		 1.32 		 3
2017 		 1.37 		 1
2018 		 1.28 		 4


SEED 2
#decrease in performance
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.33 		 11
2012 		 1.28 		 11
2013 		 1.21 		 19
2014 		 1.12 		 18
2015 		 1.15 		 16
2016 		 1.23 		 13
2017 		 1.21 		 17
2018 		 1.11 		 22

CONCLUSIONS:
Huber loss doesn't pose any definitive improvement over MSE.

DISCUSSION:

**************************************************************

C(RELU) -> C2
|
C1(Huber Loss)

HYPOTHESIS:
USE LEAKY RELU

MODEL   :   C2 : INPUT->FC1->LeakyRELU->FC2->LeakyRELU->OUT
                 LOSS = MSE LOSS


LEARNING:   INIT_WEIGHT     = normal(0.01)
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.001
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
SEED 0
YEAR		AVG_RWD		VIOLATIONS
2011 		 0.57 		 53
2012 		 0.97 		 22
2013 		 1.05 		 21
2014 		 0.79 		 37
2015 		 1.09 		 12
2016 		 1.09 		 14
2017 		 1.16 		 16
2018 		 0.96 		 24


SEED 1
2011 		 1.46 		 0
2012 		 1.4 		 0
2013 		 1.41 		 0
2014 		 1.32 		 1
2015 		 1.32 		 0
2016 		 1.35 		 0
2017 		 1.41 		 0
2018 		 1.32 		 3

SEED 2
2011 		 1.33 		 4
2012 		 1.24 		 4
2013 		 1.26 		 7
2014 		 1.13 		 6
2015 		 1.19 		 2
2016 		 1.22 		 7
2017 		 1.29 		 5
2018 		 1.2 		 6

CONCLUSIONS:
Leaky RELU can outperform RELU if intilizations are good
DISCUSSION:


**************************************************************

C(RELU) -> C2(LeakyRELU)-> C2B(LR=0.0005)
|
C1(Huber Loss)->C2B(LR=0.0005)

HYPOTHESIS:
USE LEAKY RELU

MODEL   :   C2 : INPUT->FC1->LeakyRELU->FC2->LeakyRELU->OUT
                 LOSS = MSE LOSS


LEARNING:   INIT_WEIGHT     = normal(0.01)
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.0005
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
SEED 0
#definitive improvement
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.33 		 2
2012 		 1.24 		 3
2013 		 1.34 		 2
2014 		 1.16 		 5
2015 		 1.12 		 5
2016 		 1.18 		 4
2017 		 1.22 		 7
2018 		 1.28 		 0


SEED 1
#similar if not better performance
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.46 		 0
2012 		 1.4 		 0
2013 		 1.45 		 0
2014 		 1.33 		 0
2015 		 1.32 		 0
2016 		 1.37 		 0
2017 		 1.4 		 2
2018 		 1.38 		 1

SEED 2
#worse performance as compared to LR=0.001
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.12 		 15
2012 		 1.08 		 14
2013 		 1.07 		 18
2014 		 0.81 		 28
2015 		 0.99 		 13
2016 		 1.2 		 5
2017 		 1.02 		 19
2018 		 1.0 		 14


CONCLUSIONS:
Leaky RELU may show some promise but does not show any substantial better performance.
Will not rule out Leaky RELU just yet.

DISCUSSION:


**************************************************************
C(RELU) -> C3
|
C1(Huber Loss)
|
C2(Leaky RELU)

HYPOTHESIS:
USE Xavier and Kaiming Initializations

MODEL   :   C3 : INPUT->FC1->RELU->FC2->RELU->OUT

LEARNING:   INIT_WEIGHT     = FC1 : Xavier
                              FC2 : Kaiming
                              OUT : Kaiming
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.001
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
#Slightly better than C
SEED 1

YEAR		AVG_RWD		VIOLATIONS
2011 		 1.27 		 0
2012 		 1.2 		 0
2013 		 1.2 		 1
2014 		 1.19 		 0
2015 		 1.21 		 0
2016 		 1.25 		 2
2017 		 1.28 		 0
2018 		 1.2 		 1

SEED 2
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.42 		 0
2012 		 1.33 		 0
2013 		 1.4 		 0
2014 		 1.29 		 0
2015 		 1.26 		 0
2016 		 1.34 		 0
2017 		 1.36 		 2
2018 		 1.34 		 0

SEED 3
YEAR		AVG_RWD		VIOLATIONS
2011 		 0.92 		 6
2012 		 0.89 		 5
2013 		 0.86 		 8
2014 		 0.81 		 7
2015 		 0.86 		 7
2016 		 0.93 		 9
2017 		 0.92 		 5
2018 		 0.84 		 6

CONCLUSIONS:
Results were still much better than C in spite of wrong initialization.
DISCUSSION:
The layer initializations were wrong. 
Correct order is Kaiming, Kaiming, Xavier
**************************************************************

C(RELU) -> C3(Xa-Ka-Ka)->C3A
|
C1(Huber Loss)
|
C2(Leaky RELU)


HYPOTHESIS:
Decreasing learning rate to see if the learning is better and stabler.

MODEL   :   C3A : INPUT->FC1->RELU->FC2->RELU->OUT

LEARNING:   INIT_WEIGHT     = FC1 : Xavier
                              FC2 : Kaiming
                              OUT : Kaiming
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.0001
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
# More stable learning and overall better results

SEED 1
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.13 		 4
2012 		 1.05 		 6
2013 		 1.11 		 4
2014 		 0.91 		 13
2015 		 0.9 		 12
2016 		 0.96 		 9
2017 		 1.06 		 10
2018 		 1.05 		 5


SEED 2
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.35 		 0
2012 		 1.27 		 0
2013 		 1.31 		 0
2014 		 1.22 		 0
2015 		 1.16 		 0
2016 		 1.23 		 0
2017 		 1.3 		 3
2018 		 1.26 		 0

SEED 3
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.31 		 0
2012 		 1.26 		 0
2013 		 1.29 		 0
2014 		 1.21 		 0
2015 		 1.2 		 0
2016 		 1.24 		 0
2017 		 1.3 		 0
2018 		 1.24 		 0

CONCLUSIONS:
Learning rate = 0.0001 is okay. C4A also confirms this.

DISCUSSION:

**************************************************************


C(RELU) -> C4
|
C1(Huber Loss)
|
C2(Leaky RELU)
|
C3(Xa-Ka-Ka)


HYPOTHESIS:
USE Xavier and Kaiming Initializations

MODEL   :   C4 : INPUT->FC1->RELU->FC2->RELU->OUT

LEARNING:   INIT_WEIGHT     = FC1 : Kaiming
                              FC2 : Kaiming
                              OUT : Xavier
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.001
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
SEED 0
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.45 		 0
2012 		 1.4 		 0
2013 		 1.45 		 0
2014 		 1.36 		 0
2015 		 1.36 		 0
2016 		 1.41 		 0
2017 		 1.45 		 0
2018 		 1.4 		 0

SEED 1
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.02 		 12
2012 		 1.0 		 7
2013 		 1.0 		 8
2014 		 0.87 		 17
2015 		 1.02 		 8
2016 		 1.08 		 9
2017 		 1.02 		 13
2018 		 0.95 		 11

SEED 2
YEAR		AVG_RWD		VIOLATIONS
2011 		 0.77 		 21
2012 		 0.69 		 22
2013 		 0.64 		 29
2014 		 0.45 		 38
2015 		 0.73 		 21
2016 		 0.87 		 14
2017 		 0.75 		 27
2018 		 0.64 		 28


CONCLUSIONS:
Layer initialization doesn't improve performance consistently.
No better performance than C3 (Xa,Ka,Ka) or C(normal(0.01)).

DISCUSSION:
The end results seems to be affected by the initializations. Maybe decreasing the learning rate might help rectify the problem.
**************************************************************

C(RELU) -> C4(Ka-Ka-Xa)->C4A
|
C1(Huber Loss)
|
C2(Leaky RELU)
|
C3(Xa-Ka-Ka)


HYPOTHESIS:
Decrease learning rate to see if learning is more stabler

MODEL   :   C4A : INPUT->FC1->RELU->FC2->RELU->OUT

LEARNING:   INIT_WEIGHT     = FC1 : Kaiming
                              FC2 : Kaiming
                              OUT : Xavier
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.0001
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
# Learning is slower but has much better performance than C4 and C.
# BEST PERFORMANCE SO FAR

SEED 0
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.43 		 2
2012 		 1.35 		 3
2013 		 1.47 		 0
2014 		 1.27 		 4
2015 		 1.28 		 2
2016 		 1.29 		 3
2017 		 1.39 		 5
2018 		 1.4 		 0

SEED 1
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.46 		 0
2012 		 1.37 		 1
2013 		 1.43 		 0
2014 		 1.33 		 0
2015 		 1.31 		 0
2016 		 1.39 		 0
2017 		 1.42 		 1
2018 		 1.37 		 0

SEED 2
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.5 		 0
2012 		 1.45 		 0
2013 		 1.47 		 0
2014 		 1.4 		 0
2015 		 1.38 		 0
2016 		 1.39 		 0
2017 		 1.46 		 0
2018 		 1.4 		 0

SEED 3
#Learning ends at a bad point.
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.14 		 10
2012 		 1.04 		 12
2013 		 1.05 		 14
2014 		 0.84 		 21
2015 		 0.85 		 16
2016 		 0.95 		 13
2017 		 0.98 		 20
2018 		 0.93 		 15

SEED 4
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.51 		 0
2012 		 1.43 		 0
2013 		 1.49 		 0
2014 		 1.37 		 0
2015 		 1.35 		 0
2016 		 1.41 		 0
2017 		 1.49 		 0
2018 		 1.42 		 0

SEED 5
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.39 		 0
2012 		 1.37 		 0
2013 		 1.38 		 0
2014 		 1.25 		 2
2015 		 1.24 		 0
2016 		 1.31 		 0
2017 		 1.32 		 3
2018 		 1.31 		 0

SEED 6
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.39 		 1
2012 		 1.26 		 3
2013 		 1.34 		 1
2014 		 1.2 		 3
2015 		 1.19 		 2
2016 		 1.27 		 2
2017 		 1.3 		 5
2018 		 1.26 		 0

CONCLUSIONS:
Decreasing learning rate leads to more consistent and robust learning.
Further testing with different seeds show consistently good results.

DISCUSSION:

**************************************************************

C(RELU)
|
C1(Huber Loss)
|              
C2(Leaky RELU)
|
C3(Xa-Ka-Ka)   
|              
C4(Ka-Ka-Xa)->C4A(LR=0.0001)->C4B


HYPOTHESIS:
Since LR=0.0001 had some slow learning behavior, check how learning rate to 0.0005 turns out.

MODEL   :   C4B : INPUT->FC1->LRELU->FC2->LRELU->OUT
                 LOSS : MSE

LEARNING:   INIT_WEIGHT     = FC1 : Kaiming
                              FC2 : Kaiming
                              OUT : Xavier
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.0005
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY

NOTES:      


RESULTS:
# As good as if not better than C4A.


SEED 0
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.2 		 3
2012 		 1.1 		 4
2013 		 1.19 		 0
2014 		 1.07 		 5
2015 		 1.11 		 4
2016 		 1.22 		 1
2017 		 1.26 		 2
2018 		 1.13 		 3

SEED 1
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.41 		 0
2012 		 1.38 		 0
2013 		 1.4 		 0
2014 		 1.32 		 0
2015 		 1.29 		 0
2016 		 1.34 		 0
2017 		 1.37 		 1
2018 		 1.36 		 0

SEED 2
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.42 		 0
2012 		 1.34 		 0
2013 		 1.33 		 1
2014 		 1.24 		 0
2015 		 1.26 		 0
2016 		 1.34 		 0
2017 		 1.33 		 3
2018 		 1.29 		 0

SEED 3
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.45 		 2
2012 		 1.38 		 1
2013 		 1.45 		 0
2014 		 1.29 		 3
2015 		 1.29 		 2
2016 		 1.37 		 0
2017 		 1.36 		 6
2018 		 1.38 		 0

SEED 4
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.48 		 0
2012 		 1.4 		 0
2013 		 1.43 		 0
2014 		 1.34 		 1
2015 		 1.34 		 0
2016 		 1.38 		 0
2017 		 1.4 		 3
2018 		 1.38 		 0

SEED 5
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.18 		 3
2012 		 1.07 		 7
2013 		 1.12 		 6
2014 		 1.02 		 6
2015 		 1.02 		 5
2016 		 1.13 		 3
2017 		 1.1 		 10
2018 		 1.08 		 5

SEED 6
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.12 		 0
2012 		 1.13 		 0
2013 		 1.13 		 0
2014 		 1.09 		 0
2015 		 1.07 		 0
2016 		 1.09 		 0
2017 		 1.07 		 2
2018 		 1.09 		 0


CONCLUSIONS:
LR=0.0005 seems like a good compromise for LR.

DISCUSSION:

**************************************************************
**************************************************************

C(RELU)
|
C1(Huber Loss)
|              
C2(Leaky RELU)
|
C3(Xa-Ka-Ka)   
|              
C4(Ka-Ka-Xa)-->C4A(LR=0.0001)--OK
|
C4B->C4B1

HYPOTHESIS:
Check how the performance varies if EPSILON is increase to 0.95

MODEL   :   C4B1 : INPUT->FC1->LRELU->FC2->LRELU->OUT
                   LOSS : MSE

LEARNING:   INIT_WEIGHT     = FC1 : Kaiming
                              FC2 : Kaiming
                              OUT : Xavier
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.0005
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.95              
            GAMMA           = 0.9                
            LAMBDA          = 0.9  
TRAINING:   TOKYO[2000-2009]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY
RESULTS:
#overall better performance but not very significant.

SEED 0
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.41 		 0
2012 		 1.38 		 0
2013 		 1.4 		 0
2014 		 1.32 		 0
2015 		 1.31 		 0
2016 		 1.33 		 0
2017 		 1.38 		 0
2018 		 1.35 		 0

SEED 1
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.36 		 0
2012 		 1.26 		 1
2013 		 1.34 		 1
2014 		 1.23 		 1
2015 		 1.24 		 0
2016 		 1.29 		 1
2017 		 1.32 		 3
2018 		 1.29 		 1

SEED 2
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.45 		 0
2012 		 1.38 		 0
2013 		 1.41 		 0
2014 		 1.29 		 1
2015 		 1.28 		 1
2016 		 1.34 		 0
2017 		 1.38 		 3
2018 		 1.34 		 0

SEED 3
EAR		AVG_RWD		VIOLATIONS
2011 		 1.46 		 2
2012 		 1.4 		 1
2013 		 1.43 		 1
2014 		 1.24 		 6
2015 		 1.26 		 7
2016 		 1.39 		 3
2017 		 1.47 		 2
2018 		 1.38 		 1

SEED 4
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.36 		 1
2012 		 1.34 		 1
2013 		 1.29 		 3
2014 		 1.2 		 2
2015 		 1.24 		 0
2016 		 1.28 		 3
2017 		 1.32 		 3
2018 		 1.25 		 2

SEED 5
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.44 		 0
2012 		 1.34 		 0
2013 		 1.39 		 0
2014 		 1.29 		 0
2015 		 1.29 		 0
2016 		 1.37 		 0
2017 		 1.38 		 2
2018 		 1.35 		 0


SEED 6
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.5 		 0
2012 		 1.44 		 0
2013 		 1.5 		 0
2014 		 1.41 		 0
2015 		 1.38 		 0
2016 		 1.42 		 0
2017 		 1.47 		 0
2018 		 1.42 		 0


CONCLUSIONS:
Slightly Better performance.

DISCUSSION:
Better performance for reduced robustness?
**************************************************************